---
title: "MA Thesis Code Checks"
author: "Kyle Dewsnap"
date: "28/09/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```
As the algorithm that implements the OLS estimator is manually coded, it is wise to check the output of this algorithm against both a worked example and against other, audited statistics software.

## Validation against Worked Example

Let:

- $X$ be an $n * k$ design matrix, where $n$ is the number of observations and $k$ is the number of predictor variables _plus_ one intercept term (the leftmost column of $X$ is filled with ones);
- $\beta$ be a vector of length $k$ representing the true model parameters, and $\hat{\beta}$ be its estimates;
- $\epsilon$ be a vector of length $n$ representing the error terms of the observations, where $\epsilon \sim N(0, \sigma^2)$, and;
- $y$ be a vector of length $n$ representing the actual values of the response variable, and $\hat{y}$ be its estimates from the model.

For this example, we set $n = 4$ and $k = 3$ (namely, we have two predictor variables, $X_1$ and $X_2$). Both predictor variables follow a random distribution: $X_1 \sim N(0, 1), X_2 \sim N(1, 1)$.
$$
X = \left(\begin{array}{ccc}
1 & 0.7 & 2.0 \\
1 & -1.0 & 0.4 \\
1 & 0.7 & 1.5 \\
1 & 0.6 & 0.3 \end{array}\right)\
$$
The true model used to generate the response variable is
$$
\begin{equation}
\begin{split}
Y & = X\beta + \epsilon \\
& = X \left(\begin{array}{c}0.5 \\ 1 \\ -0.5 \end{array}\right)\ + \left(\begin{array}{c}0.2 \\ 1.8 \\ -1.1 \\ 0.0 \end{array}\right)\ \\
& = \left(\begin{array}{c}0.4 \\ 1.1 \\ -0.6 \\ 1.0 \end{array}\right)\
\end{split}
\end{equation}
$$
The OLS estimator of $\beta$ is $\hat{\beta} = (X'X)^{-1}X'y$, where $'$ is the transpose operator. Given our data, the parameter estimates of this model are:
$$
\begin{equation}
\begin{split}
\hat{\beta} & = (X'X)^{-1}X'y \\
 & = \left(\begin{array}{ccc}
4.00	& 1.00 &	4.20 \\
1.00 &	2.34 & 2.23 \\
4.20 & 2.23 & 6.50 \end{array}\right)^{-1}
\left(\begin{array}{cccc}
1 & 1 & 1 & 1 \\
0.7 & -1 & 0.7 & 0.6 \\
2 & 0.4 & 1.5 & 0.3 \end{array}\right)
\left(\begin{array}{c}0.4 \\ 1.1 \\ -0.6 \\ 1.0 \end{array}\right)\ \\
 & = \left(\begin{array}{ccc}
0.860 & 0.241 & -0.638 \\
0.241 & 0.702 & -0.397 \\
-0.638 & -0.397 & -0.702
 \end{array}\right)
 \left(\begin{array}{c}1.90 \\ -0.64 \\ 0.64 \end{array}\right)\ \\
 & = \left(\begin{array}{c}1.071 \\ -0.246 \\ -0.510 \end{array}\right)\
\end{split}
\end{equation}
$$
This test was passed; see `LmOLS_Test.cpp:26-28'.

The standard errors of $\hat{\beta}$ are given by the squared diagonal of the variance-covariance matrix of the OLS estimates. This is given by
$E[(\hat{\beta} - \beta)(\hat{\beta} - \beta)'] = \hat{\sigma^2}(X'X)^(-1)$, where $\hat{\sigma^2} = \frac{RSS}{n - k}$. Given our data, the standard errors of the parameter estimates of this model are:
$$
\begin{equation}
\begin{split}
\hat{\sigma} & = \frac{RSS}{n - k} \\
 & = \frac{\sum_{i=0}^n\hat{y}_i-y_i}{n-k} \\
 & = \frac{0.863}{4-3} \\
 & = 0.863 \\
E[(\hat{\beta} - \beta)(\hat{\beta} - \beta)'] & = \hat{\sigma^2}(X'X)^(-1) \\
 & = 0.863^2 \left(\begin{array}{ccc}
0.860 & 0.241 & -0.638 \\
0.241 & 0.702 & -0.397 \\
-0.638 & -0.397 & -0.702 \end{array}\right) \\
 & = \left(\begin{matrix}
0.640 & 0.179 & -0.475 \\
0.179 & 0.523 & -0.295 \\
-0.475 & -0.295 & 0.523 \end{matrix}\right) \\
SE(\hat{\beta}) & = \left(\begin{array}{c} \sqrt{0.640} \\ \sqrt{0.523} \\ \sqrt{0.523} \end{array}\right) \\
 & = \left(\begin{array}{c}0.800 \\ 0.723 \\ 0.723 \end{array}\right)
\end{split}
\end{equation}
$$
This test was passed; see `LmOLS_Test.cpp:31-34`.

The coefficient of determination of the model is given by $R^2 = 1 - \frac{SSR}{SST}$. Given our data, the coefficient of determination is:
$$
\begin{equation}
\begin{split}
R^2 & = 1 - \frac{SSR}{SST} \\
 & = 1 - \frac{0.863}{1.828} \\
 & = 0.528
\end{split}
\end{equation}
$$
This test was passed; see `LmOLS_Test.cpp:36`.

## Validation against R Statistics Package and Existing Regression Analysis Results

Henderson and Velleman (1981) present exemplars of regression analyses with two different datasets; one dataset, extracted from the 1974 _Motor Trend_ magazine, has become a canonical dataset within statistics packages for teaching and algorithm validation purposes. This data, which compresses fuel consumption and 10 different aspects of automotive design for 32 cars, was used by the original authors to illustrate their model building procedure in examining the relationship between gasoline mileage in gallons per 100 miles (GPM), car weight in thousands of pounds (WT), and horsepower (HP). Henderson and Velleman settled on the following model:
$$
GPM = \beta_0 + \beta_{WT}WT + \beta_{HP/WT}HP/WT + \epsilon
$$
where $\epsilon \sim N(0,\sigma^2)$. This dataset has been included within the basic distribution of the R statistical programming software as `mtcars`.

We will test our implementation of the OLS estimator in the following way: First, we will prepare a design matrix containing both $WT$ and $HP/WT$ variables, alongside a $GPM$ vector in formats that are legible to C++. Using these data, both our implementation and R will produce estimates of $\beta$, in addition to a coefficient of determination. The R implementation of statistical procedures in the base package `stats` has been audited for use in regulated clinical trial environments, where it was found that it provided reliable parameter estimates for generalized linear models (The R Foundation for Statistical Computing, 2018). We will compare the output of our C++ implementation of OLS against the output of R in addition to the estimates provided in Henderson and Velleman (1981) to check for correspondence.

```{r}
test_df <- mtcars %>%
        transmute(
                GPG = (1/mpg)*100, #  Gallons per Mile = (Miles per Gallon)^-1, 
                WT = wt,
                HP_WT = hp / wt
        )

head(test_df)

r_mod <- lm(GPG ~ WT + HP_WT, data = test_df)
```
The estimates and the coefficient of determinations for this model are identical between the C++ implementation (see `LmOLS_Test.cpp:105-118`), the R implementation of linear model fitting, and the original paper. 

We note, however, that the standard errors of the estimates from the C++ implementation are smaller than the estimates provided by R and the paper. This is because both R and the paper used maximum likelihood estimation (in the form of the iterative reweighed least squares) to estimate the model's parameters. While we expect the estimates produced by MLE and OLS to be identical (recall that the ML estimator for the parameters of a linear model and OLS are identical), we also expect that the standard errors of the estimates will be smaller from OLS than from MLE due to the Gauss-Markov theorem (specifically, MLE estimates of $\sigma^2$ are biased with small sample sizes, whereas the estimates produced by OLS are not).